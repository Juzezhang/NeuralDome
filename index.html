<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>NeuralDome</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class=>CVPR 2023 </h1>
      <h1 class="project-name">NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions</h1>

      <h2 class="project-tagline">Juze Zhang<sup>1,2,3,4</sup>* &emsp; Haimin Luo<sup>1,4</sup>* &emsp; Hongdi Yang<sup>1,4</sup> &emsp; Xinru Xu<sup>1,4</sup> &emsp; Qianyang Wu<sup>1,4</sup> &emsp; Ye Shi<sup>1,4</sup> &emsp; Jingyi Yu<sup>1,4</sup> &emsp; Lan Xu<sup>1,4</sup>† &emsp; Jingya Wang<sup>1,4,</sup>†</h2>
      <h2 class="project-tagline"><sup>1</sup>ShanghaiTech University &emsp; <sup>2</sup>Shanghai Advanced Research Institute, Chinese Academy of Sciences </h2>
      <h2 class="project-tagline"><sup>3</sup>University of Chinese Academy of Sciences &emsp;<sup>4</sup>Shanghai Engineering Research Center of Intelligent Vision and Imaging</h2>

      <a href="https://arxiv.org/pdf/2212.07626.pdf" class="btn">paper</a>
      <a href="https://github.com/Juzezhang/IKOL" class="btn">dataset</a>
      <a href="" class="btn">slides</a>
      <a href="" class="btn">poster</a>
    </section>

    <section class="main-content">
      <p>In this paper, we construct a dense multi-view dome to acquire a complex human object interaction dataset, named HODome, that consists of ∼71M frames on 10 subjects interacting with 23 objects. To process the HODome dataset, we develop NeuralDome, a layer-wise neural processing pipeline tailored for multi-view video inputs to conduct accurate tracking, geometry reconstruction and free-view rendering, for both human subjects and objects. </p>

      <p><img src="NeuralDome.png" alt="" style="max-width:100%;"></p>

      
      <h1>
      <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Video</h1>
        
      <div class="video"><iframe class="video" frameborder="0" width="100%" height="500"
        src="https://www.youtube.com/watch?v=RvaC7w-rIhg&t=5s">
      </iframe></div>

      <h1>
      <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h1>
      If you find this repository useful in your research, please consider citing:
      <p><div class="highlight highlight-Javascript"><pre>
      @inproceedings{
      zhang2023neuraldome,
      title={NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions},
      author={Juze Zhang and Haimin Luo and Hongdi Yang and Xinru Xu and Qianyang Wu and Ye Shi and Jingyi Yu and Lan Xu and Jingya Wang},
      booktitle={CVPR},
      year={2023},
      };
      </pre></div></p>

      <footer class="site-footer">
        <!-- <span class="site-footer-owner"><a href="https://github.com/jasonlong/cayman-theme">Cayman</a> is maintained by <a href="https://github.com/jasonlong">jasonlong</a>.</span> -->
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>

    </section>

  </body>
</html>
